{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba83d5ee-333e-4ac2-9cb6-fd975cdf3f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 17:43:02.645686: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-12 17:43:02.655048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747071782.667041  178234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747071782.670539  178234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-12 17:43:02.682677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "from utils.llm_configs import setup_llm, load_column_mapping\n",
    "from utils.context import extract_surrounding_context\n",
    "from utils.candidates import build_alias_kb, retrieve_candidates\n",
    "from utils.prompts_utils import construct_pointwise_prompt, parse_llm_decision\n",
    "from utils.io import safe_parse_candidates, save_intermediate_outputs\n",
    "from utils.EL_eval import compute_metrics_from_pointwise_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9277b30-b579-4361-b741-38bba2dc1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(path: str, column_mapping: dict) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, dtype={'gt_wiki_id': 'Int64'})\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    df.dropna(subset=['entity_title'], inplace=True)\n",
    "    df = df[df['wiki_ID'] != -1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d73c5c-29f1-4dc7-b9d6-c316f1aa696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_contexts(df: pd.DataFrame, n: int = 2) -> pd.DataFrame:\n",
    "    df['surrounding_context'] = df.apply(\n",
    "        lambda row: extract_surrounding_context(row['article_text'], row['offsets'], row['entity_title'], n=n), axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a81f70f-ee44-4eb2-9cd2-1404a7f7646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_candidates(df: pd.DataFrame, alias_kb: dict) -> pd.DataFrame:\n",
    "    df['candidates'] = df['entity_title'].apply(lambda x: retrieve_candidates(x, alias_kb))\n",
    "    df['pre_pt_len_candidates'] = df['candidates'].apply(lambda c: len(c) if c else 0)\n",
    "    df.dropna(subset=['candidates'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e98569-b9ac-48d4-9e18-f467c858f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_dataframe(df: pd.DataFrame, model: str) -> pd.DataFrame:\n",
    "    prompt_records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        for cand_idx, cand in enumerate(row['candidates']):\n",
    "            prompt = construct_pointwise_prompt(row, cand, model=model)\n",
    "            prompt_records.append({\n",
    "                \"row_idx\": idx,\n",
    "                \"cand_idx\": cand_idx,\n",
    "                \"messages\": prompt\n",
    "            })\n",
    "    return pd.DataFrame(prompt_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716c0985-1704-4aac-857d-a7f0006c6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(llm, prompt_df: pd.DataFrame, model: str, sampling_params, batch_size: int = 5000, checkpoint_path: str = None):\n",
    "    all_outputs = []\n",
    "    seen = set()\n",
    "\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        existing_df = pd.read_csv(checkpoint_path)\n",
    "        seen = {(r['row_idx'], r['cand_idx']) for _, r in existing_df.iterrows()}\n",
    "        all_outputs.extend(existing_df.to_dict('records'))\n",
    "        print(f\"[Resume] Loaded {len(all_outputs)} previously completed records.\")\n",
    "\n",
    "    for chunk_start in range(0, len(prompt_df), batch_size):\n",
    "        chunk = prompt_df.iloc[chunk_start:chunk_start + batch_size]\n",
    "        if all((row['row_idx'], row['cand_idx']) in seen for _, row in chunk.iterrows()):\n",
    "            continue\n",
    "            \n",
    "        responses = llm.chat(messages=list(chunk['messages']), sampling_params=sampling_params)\n",
    "        batch_outputs = []\n",
    "\n",
    "        for i, resp in enumerate(responses):\n",
    "            row = chunk.iloc[i]\n",
    "            txt = resp.outputs[0].text\n",
    "            keep = parse_llm_decision(txt, model=model)\n",
    "            out = {\n",
    "                \"row_idx\": row['row_idx'],\n",
    "                \"cand_idx\": row['cand_idx'],\n",
    "                \"keep\": keep,\n",
    "                \"llm_text\": txt\n",
    "            }\n",
    "            all_outputs.append(out)\n",
    "            batch_outputs.append(out)\n",
    "\n",
    "        if checkpoint_path:\n",
    "            pd.DataFrame(batch_outputs).to_csv(\n",
    "                checkpoint_path,\n",
    "                mode='a',\n",
    "                header=not os.path.exists(checkpoint_path),\n",
    "                index=False\n",
    "            )\n",
    "            print(f\"[Checkpoint] Saved batch {chunk_start // batch_size + 1}\")\n",
    "\n",
    "    return pd.DataFrame(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636beb0b-5d71-4e1d-a24a-4e7ef8d52d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_candidates(df: pd.DataFrame, output_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['candidates_after_pointwise'] = df['candidates'].apply(lambda x: copy.deepcopy(x))\n",
    "    for _, row in output_df.iterrows():\n",
    "        df.at[row['row_idx'], 'candidates_after_pointwise'][row['cand_idx']]['llm_decision'] = row['llm_text']\n",
    "        df.at[row['row_idx'], 'candidates_after_pointwise'][row['cand_idx']]['relevant'] = row['keep']\n",
    "\n",
    "    df['candidates_after_pointwise'] = df['candidates_after_pointwise'].apply(\n",
    "        lambda cands: [c for c in cands if c.get(\"relevant\") is True] if isinstance(cands, list) else []\n",
    "    )\n",
    "    df['post_pt_len_candidates'] = df['candidates_after_pointwise'].apply(lambda x: len(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe03e478-b81b-4fdf-812b-65551ecf35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = \"llama\"  # or \"zephyr\"\n",
    "    llm, sampling_params = setup_llm(model=model)\n",
    "    column_mapping = load_column_mapping()\n",
    "\n",
    "    sed_outputs = prepare_dataset(\"/work/pi_wenlongzhao_umass_edu/8/696-detecting-salient-entities/results/sed_results/SED_output.csv\", column_mapping)\n",
    "\n",
    "    with open(\"prep_kb/filtered_kb_4_22.json\") as f:\n",
    "        kb = json.load(f)\n",
    "    alias_kb = build_alias_kb(kb)\n",
    "\n",
    "    sed_outputs_subset = sed_outputs.head(100).copy()\n",
    "    sed_outputs_subset = extract_all_contexts(sed_outputs_subset, n=2)\n",
    "    sed_outputs_subset = assign_candidates(sed_outputs_subset, alias_kb)\n",
    "    \n",
    "    prompt_df = build_prompt_dataframe(sed_outputs_subset, model=model)\n",
    "    print('Length of prompts: ', len(prompt_df))\n",
    "\n",
    "    checkpoint_path = f\"outputs/pointwise/checkpoints/{model}_checkpoint.csv\"\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    output_df = run_inference(llm, prompt_df, model=model, sampling_params=sampling_params, checkpoint_path=checkpoint_path)\n",
    "\n",
    "    output_df.drop_duplicates(subset=['row_idx', 'cand_idx'], inplace=True)\n",
    "    sed_outputs_subset = update_candidates(sed_outputs_subset, output_df)\n",
    "    save_intermediate_outputs(sed_outputs_subset, f\"outputs/pointwise/final/intermediate_results_{model}.csv\")\n",
    "\n",
    "    metrics = compute_metrics_from_pointwise_csv(f\"outputs/pointwise/final/intermediate_results_{model}.csv\")\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba39977a-f3b0-4023-9695-36c092cc72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 17:43:09 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-12 17:43:20 config.py:549] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-12 17:43:20 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 05-12 17:43:20 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 05-12 17:43:20 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f', speculative_config=None, tokenizer='/datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/work/pi_wenlongzhao_umass_edu/8/aranade/models', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-12 17:43:21 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-12 17:43:21 model_runner.py:1110] Starting to load model /datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02d9d2785f24fa1aeecda86bfcc7279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 17:43:24 model_runner.py:1115] Loading model weights took 14.9888 GB\n",
      "INFO 05-12 17:43:25 worker.py:267] Memory profiling takes 0.59 seconds\n",
      "INFO 05-12 17:43:25 worker.py:267] the current vLLM instance can use total_gpu_memory (44.40GiB) x gpu_memory_utilization (0.90) = 39.96GiB\n",
      "INFO 05-12 17:43:25 worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 23.70GiB.\n",
      "INFO 05-12 17:43:25 executor_base.py:111] # cuda blocks: 12136, # CPU blocks: 2048\n",
      "INFO 05-12 17:43:25 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 1.48x\n",
      "INFO 05-12 17:43:26 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 17:43:44 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.26 GiB\n",
      "INFO 05-12 17:43:44 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of prompts:  6240\n",
      "INFO 05-12 17:43:50 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5000/5000 [06:01<00:00, 13.82it/s, est. speed input: 10875.75 toks/s, output: 716.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[decision warning] Falling back to YES — could not parse: {\"final_decision\":\"yes\",\"reasoning\":\"The mention 'Japan' modifies the wheelchair basketball team, which is plausible giv — Expecting ',' delimiter: line 1 column 51 (char 50)\n",
      "[decision warning] Falling back to YES — could not parse: {\"final_decision\":\"no\",\"reasoning\":\"The candidate refers to a specific event at the 2004 Summer Olympics, whereas the co — Expecting ',' delimiter: line 1 column 227 (char 226)\n",
      "[decision warning] Falling back to YES — could not parse: {\"final_decision\":\"no\",\"reasoning\":\"The context is about artistic gymnastics, and the Olympic Games in London, whereas t — Expecting ',' delimiter: line 1 column 236 (char 235)\n",
      "[decision warning] Falling back to YES — could not parse: {\"final_decision\":\"no\",\"reasoning\":\"The candidate refers to the 1988 Summer Olympics, which took place in Seoul, South K — Expecting ',' delimiter: line 1 column 276 (char 275)\n",
      "[decision warning] Falling back to YES — could not parse: {\"final_decision\":\"no\",\"reasoning\":\"The context is discussing gymnastics, not diving, and the mention of the Olympics is — Expecting ',' delimiter: line 1 column 249 (char 248)\n",
      "[decision warning] Falling back to YES — could not parse: {\"final_decision\":\"no\",\"reasoning\":\"The candidate refers to a sports team in the context of rugby league, which is not r — Expecting ',' delimiter: line 1 column 242 (char 241)\n",
      "[Checkpoint] Saved batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1240/1240 [01:33<00:00, 13.28it/s, est. speed input: 10610.28 toks/s, output: 712.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved batch 2\n",
      "{'Recall@filter': 0.8837209302325582, 'Reduction ratio': 0.5243734219871514, 'Average retained candidates': 27.930232558139537}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f098fc-3533-4abf-9200-97c1372df87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline-salient-entities",
   "language": "python",
   "name": "baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
