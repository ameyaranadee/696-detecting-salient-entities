{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7232f69-2535-45ef-8c72-64229ea44b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import glob\n",
    "# import spacy\n",
    "# import pickle\n",
    "# import random\n",
    "# import difflib\n",
    "# import textwrap\n",
    "# import datetime\n",
    "# import jsonlines\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from utils.shared_configs import LLAMA_MODEL_PATH, ZEHPYR_MODEL_PATH, get_sampling_params, initialize_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37ba7fe-3703-4563-b339-ab2e85a4cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:25:21.640989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-17 20:25:21.650582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747513521.662773 2852962 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747513521.666317 2852962 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-17 20:25:21.678649: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from utils.prompts_utils import construct_contextual_prompt, parse_contextual_el_output, COT_POOL\n",
    "from utils.llm_configs import setup_llm, get_sampling_params\n",
    "from utils.io import save_intermediate_outputs\n",
    "from utils.EL_eval import evaluate_contextual_linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a5ffef-8be0-4479-94f7-beaf24884b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contextual_prompts(df: pd.DataFrame, model: str = \"llama\"):\n",
    "    prompt_records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if not row['candidates_after_pointwise']:\n",
    "            continue\n",
    "        prompt = construct_contextual_prompt(row, model=model)\n",
    "        label_map = {i+1: cand for i, cand in enumerate(row['candidates_after_pointwise'])}\n",
    "        prompt_records.append((idx, label_map, prompt))\n",
    "    return prompt_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff29f936-c855-4405-8869-6ec7c649d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_contextual_inference(prompt_records, llm, sampling_params, model, batch_size=1000):\n",
    "    all_outputs = []\n",
    "    for chunk_start in range(0, len(prompt_records), batch_size):\n",
    "        batch = prompt_records[chunk_start:chunk_start + batch_size]\n",
    "        prompts = [rec[2] for rec in batch]\n",
    "        if model == \"llama\":\n",
    "            responses = llm.chat(messages=prompts, sampling_params=sampling_params)\n",
    "        # else:\n",
    "        #     responses = llm.generate(prompts=prompts, sampling_params=sampling_params)\n",
    "\n",
    "        for (record, response) in zip(batch, responses):\n",
    "            idx, label_map, _ = record\n",
    "            text = response.outputs[0].text.strip()\n",
    "            selected_label = parse_contextual_el_output(text)\n",
    "\n",
    "            selected_candidate = label_map.get(selected_label, 0)\n",
    "            all_outputs.append((idx, selected_candidate))\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1e255c-9b59-4506-8dbd-2623805426ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_linked_entities(df, all_outputs):\n",
    "    top_linked_entities = [0] * len(df)\n",
    "    for idx, selected_candidate in all_outputs:\n",
    "        if selected_candidate:\n",
    "            top_linked_entities[idx] = int(selected_candidate['wiki_id'])\n",
    "    df['top_linked_entity'] = pd.Series(top_linked_entities, dtype=\"Int64\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0694478a-c21b-40ee-9f7b-e84a3f3d9aa9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = \"llama\"  # or \"zephyr\"\n",
    "    sampling_params = get_sampling_params(max_tokens=350, temperature=0.6, top_p=0.9, stops=[\"</s>\", \"\\n}\"])\n",
    "    llm, sampling_params = setup_llm(model=model)\n",
    "\n",
    "    df = pd.read_csv(f\"outputs/pointwise/final/intermediate_results_{model}_500_v2.csv\", dtype={'wiki_ID': 'Int64'})\n",
    "    df['candidates_after_pointwise'] = df['candidates_after_pointwise'].apply(\n",
    "        lambda x: ast.literal_eval(x) if pd.notna(x) else []\n",
    "    )\n",
    "\n",
    "    prompt_records = build_contextual_prompts(df, model=model)\n",
    "\n",
    "    all_outputs = run_contextual_inference(prompt_records, llm, sampling_params, model)\n",
    "    df = write_linked_entities(df, all_outputs)\n",
    "\n",
    "    save_intermediate_outputs(df, \"outputs/contextual/contextual_linked_results.csv\")\n",
    "    metrics = evaluate_contextual_linking(df)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046b47fb-369e-4b47-85f6-7690cb7e2456",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-17 20:25:26 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 05-17 20:25:36 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-17 20:25:36 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 05-17 20:25:36 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 05-17 20:25:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f', speculative_config=None, tokenizer='/datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/work/pi_wenlongzhao_umass_edu/8/aranade/models', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-17 20:25:38 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 05-17 20:25:38 model_runner.py:1110] Starting to load model /datasets/ai/llama3/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e04b0fc064196b5735cbbc6adefce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-17 20:25:41 model_runner.py:1115] Loading model weights took 14.9888 GB\n",
      "INFO 05-17 20:25:42 worker.py:267] Memory profiling takes 0.56 seconds\n",
      "INFO 05-17 20:25:42 worker.py:267] the current vLLM instance can use total_gpu_memory (44.40GiB) x gpu_memory_utilization (0.90) = 39.96GiB\n",
      "INFO 05-17 20:25:42 worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 23.70GiB.\n",
      "INFO 05-17 20:25:42 executor_base.py:111] # cuda blocks: 12136, # CPU blocks: 2048\n",
      "INFO 05-17 20:25:42 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 1.48x\n",
      "INFO 05-17 20:25:43 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-17 20:26:00 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.26 GiB\n",
      "INFO 05-17 20:26:00 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 18.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-17 20:26:01 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 465/465 [01:59<00:00,  3.90it/s, est. speed input: 8584.95 toks/s, output: 467.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[contextual warning] Could not parse: {\"final_decision\": \"1.London – London is the capital and largest city of England and the United Kingdom.\", \"reasoning\":  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: Based on the provided context and the mention of \"Olympic Games\", I will apply the entity disambiguation rules.\n",
      "\n",
      "1. Cont — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"Olympic Games\", let's analyze the context and candidates provided.\n",
      "\n",
      "The context is: \"The Pu — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"Colombia\", I will follow the steps you provided:\n",
      "\n",
      "1. Context: The text mentions the London  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"North Korea\" in the given context, let's follow the steps:\n",
      "\n",
      "1. Context: The context is abou — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\n",
      "\"final_decision\": \"2.Olympic Games\",\n",
      "\"reasoning\": \"The context is about the men's team final in artistic gymnastics at — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Illinois###\", I will consider the context, categories, modifiers, co-references, tempora — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To make a decision, I will consider the context, categories, modifiers, co-references, temporal and geographical factors — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention '###Mitt Romney###', I will follow the steps you provided:\n",
      "\n",
      "1.  **Context:** The text mentio — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.Andre Nigel Barnett\", \"reasoning\": \"Context: The text mentions the presidential nomination of the  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.2012 Summer Paralympics\", \"reasoning\": \"Context: The mention is within a sentence discussing the u — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Liberia###\", I will follow the steps you provided:\n",
      "\n",
      "1. **Context**: The text is about th — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Olympics###\", I will follow the steps you provided.\n",
      "\n",
      "1. Context: The mention is in a tex — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###2012 Summer Paralympics###\", I will follow the steps you provided:\n",
      "\n",
      "1. **Context**: The  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention '###Africa###', I will follow the given steps:\n",
      "\n",
      "1. Context: The text discusses the IPC, disa — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Cuba###\", I will follow the steps you provided:\n",
      "\n",
      "1. Context: The text mentions that Colo — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To determine the correct entity, let's follow the steps you provided:\n",
      "\n",
      "1. **Context**: The text mentions the 2012 Summer — No parseable candidate index found\n",
      "[contextual warning] Could not parse: Based on the context and the information provided, I will make a decision about the mention \"###Australia###\". \n",
      "\n",
      "From th — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###United States###\", I will follow the steps you provided:\n",
      "\n",
      "1. Context: The text is about  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###London###\", I will follow the steps you provided:\n",
      "\n",
      "1. **Context**: The context is a spor — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention '###Azerbaijan###', I will follow the steps you provided:\n",
      "\n",
      "1. Context: The text is about the — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"Cuba\", I will follow the steps you provided.\n",
      "\n",
      "1. Context: The context is about the 2012 Sum — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###London###\", let's apply the steps you provided:\n",
      "\n",
      "1. **Context:** The context mentions \"L — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###London###\", let's follow the steps you provided:\n",
      "\n",
      "1. Context: The text is about a table  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention '###Britain###', I will consider the context, categories, modifiers, co-references, temporal — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.China – China (), officially the People's Republic of China (PRC), is a country in East Asia and t — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###London###\", I will follow the steps you provided:\n",
      "\n",
      "1. **Context:** The mention \"###Londo — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To determine the correct entity, let's go through the steps of entity disambiguation:\n",
      "\n",
      "1. **Context**: The text mentions — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"United States\" in the given context, let's follow the steps:\n",
      "\n",
      "1. **Context**: The context i — No parseable candidate index found\n",
      "[contextual warning] Could not parse: ###Saudi Arabia### is mentioned in the context of the Syrian Electronic Army (SEA) attacking the websites of Al Arabiya  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Mitt Romney###\", I will follow the steps you provided:\n",
      "\n",
      "1. **Context**: The text mention — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"United States\", let's go through the steps:\n",
      "\n",
      "1. **Context**: The text mentions the United S — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"United Kingdom\" in the given context, I will follow the steps outlined:\n",
      "\n",
      "1. Context: The te — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To determine which entity the mention \"Afghanistan\" refers to, I will follow the steps you provided:\n",
      "\n",
      "1. **Context**: Th — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Turkey###\", let's follow the steps you provided:\n",
      "\n",
      "1. Context: The text is about a 5-a-si — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.Haiti – Haiti (; ; ), officially the Republic of Haiti (; ) and formerly called Hayti, is a countr — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention '###Tunisia###', I will follow the steps you provided.\n",
      "\n",
      "1. Context: The text discusses the L — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.Taliban – The Taliban (, \\\"students\\\") or Taleban, who refer to themselves as the Islamic Emirate  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.United States – The United States of America (USA), commonly known as the United States (U.S. or U — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Azerbaijan###\", I will follow the steps you provided:\n",
      "\n",
      "1. Context: The context is about  — No parseable candidate index found\n",
      "[contextual warning] Could not parse: {\"final_decision\": \"1.United Kingdom – {{About-distinguish2|the country|Great Britain, its largest island whose name is  — Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "[contextual warning] Could not parse: To determine the correct entity, let's follow the steps you provided:\n",
      "\n",
      "1. Context: The text is discussing a government a — No parseable candidate index found\n",
      "[contextual warning] Could not parse: To disambiguate the mention \"###Texas###\", I will follow the steps you provided:\n",
      "\n",
      "1. Context: The text discusses a growi — No parseable candidate index found\n",
      "Entities evaluated: 500\n",
      "Ground truth (non-null): 500\n",
      "Predictions made: 420\n",
      "Correct links: 322\n",
      "\n",
      "{'Accuracy': 0.644, 'Precision': 0.7666666666666667, 'Recall': 0.644, 'F1 Score': 0.7}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c347e3d3-76e8-4b6c-8164-7be725dee03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   article_text                500 non-null    object\n",
      " 1   date                        500 non-null    object\n",
      " 2   article_title               500 non-null    object\n",
      " 3   entity_salience             500 non-null    int64 \n",
      " 4   offsets                     500 non-null    object\n",
      " 5   wiki_ID                     500 non-null    int64 \n",
      " 6   entity_title                500 non-null    object\n",
      " 7   surrounding_context         500 non-null    object\n",
      " 8   candidates                  500 non-null    object\n",
      " 9   pre_pt_len_candidates       500 non-null    int64 \n",
      " 10  candidates_after_pointwise  500 non-null    object\n",
      " 11  post_pt_len_candidates      500 non-null    int64 \n",
      " 12  top_linked_entity           500 non-null    int64 \n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 50.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"outputs/contextual/contextual_linked_results.csv\")\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790fb43e-910c-4252-bed3-c6973c515d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1['article_title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c2c993-f5f7-4e8a-8a70-df9753281d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>entity_salience</th>\n",
       "      <th>offsets</th>\n",
       "      <th>wiki_ID</th>\n",
       "      <th>entity_title</th>\n",
       "      <th>surrounding_context</th>\n",
       "      <th>candidates</th>\n",
       "      <th>pre_pt_len_candidates</th>\n",
       "      <th>candidates_after_pointwise</th>\n",
       "      <th>post_pt_len_candidates</th>\n",
       "      <th>top_linked_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>Australian Gliders glide past China women's na...</td>\n",
       "      <td>1</td>\n",
       "      <td>(50, 60)</td>\n",
       "      <td>4689264</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>[{'mentions': 'australia', 'wiki_id': 4689264,...</td>\n",
       "      <td>93</td>\n",
       "      <td>[{'mentions': 'australia', 'wiki_id': 4689264,...</td>\n",
       "      <td>27</td>\n",
       "      <td>20611325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>Australian Gliders glide past China women's na...</td>\n",
       "      <td>1</td>\n",
       "      <td>(78, 85)</td>\n",
       "      <td>5405</td>\n",
       "      <td>China</td>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>[{'mentions': 'china', 'wiki_id': 5405, 'title...</td>\n",
       "      <td>85</td>\n",
       "      <td>[{'mentions': 'china', 'wiki_id': 5405, 'title...</td>\n",
       "      <td>35</td>\n",
       "      <td>887850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>Australian Gliders glide past China women's na...</td>\n",
       "      <td>1</td>\n",
       "      <td>(14, 29)</td>\n",
       "      <td>21654</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Homebush Bay, ###New South Wales### —Earlier t...</td>\n",
       "      <td>[{'mentions': 'new south wales', 'wiki_id': 21...</td>\n",
       "      <td>76</td>\n",
       "      <td>[{'mentions': 'new south wales', 'wiki_id': 21...</td>\n",
       "      <td>37</td>\n",
       "      <td>21654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Border patrols at Britain's airports may be le...</td>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>UK border officers go on Olympic strike</td>\n",
       "      <td>1</td>\n",
       "      <td>(584, 597)</td>\n",
       "      <td>419342</td>\n",
       "      <td>David Cameron</td>\n",
       "      <td>PCS members working in the Home Office, which ...</td>\n",
       "      <td>[{'mentions': 'david cameron', 'wiki_id': 4193...</td>\n",
       "      <td>11</td>\n",
       "      <td>[{'mentions': 'david cameron', 'wiki_id': 4193...</td>\n",
       "      <td>9</td>\n",
       "      <td>419342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>2012-07-21</td>\n",
       "      <td>China women's national wheelchair basketball t...</td>\n",
       "      <td>1</td>\n",
       "      <td>(137, 158)</td>\n",
       "      <td>848348</td>\n",
       "      <td>wheelchair basketball</td>\n",
       "      <td>Homebush Bay, New South Wales —Earlier today, ...</td>\n",
       "      <td>[{'mentions': 'wheelchair basketball', 'wiki_i...</td>\n",
       "      <td>23</td>\n",
       "      <td>[{'mentions': 'wheelchair basketball', 'wiki_i...</td>\n",
       "      <td>19</td>\n",
       "      <td>848348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>Philippines seeks United Nations arbitration o...</td>\n",
       "      <td>1</td>\n",
       "      <td>(782, 787)</td>\n",
       "      <td>106539</td>\n",
       "      <td>ASEAN</td>\n",
       "      <td>Disputes such as those involving the Scarborou...</td>\n",
       "      <td>[{'mentions': 'asean', 'wiki_id': 28741, 'titl...</td>\n",
       "      <td>8</td>\n",
       "      <td>[{'mentions': 'asean', 'wiki_id': 28741, 'titl...</td>\n",
       "      <td>6</td>\n",
       "      <td>106539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>Philippines seeks United Nations arbitration o...</td>\n",
       "      <td>1</td>\n",
       "      <td>(771, 777)</td>\n",
       "      <td>25734</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>Disputes such as those involving the Scarborou...</td>\n",
       "      <td>[{'mentions': 'taiwan', 'wiki_id': 25734, 'tit...</td>\n",
       "      <td>84</td>\n",
       "      <td>[{'mentions': 'taiwan', 'wiki_id': 25734, 'tit...</td>\n",
       "      <td>51</td>\n",
       "      <td>25734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>Philippines seeks United Nations arbitration o...</td>\n",
       "      <td>1</td>\n",
       "      <td>(194, 209)</td>\n",
       "      <td>74209</td>\n",
       "      <td>South China Sea</td>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>[{'mentions': 'south china sea', 'wiki_id': 74...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'mentions': 'south china sea', 'wiki_id': 74...</td>\n",
       "      <td>3</td>\n",
       "      <td>74209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>Philippines seeks United Nations arbitration o...</td>\n",
       "      <td>1</td>\n",
       "      <td>(4, 15)</td>\n",
       "      <td>23440</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>The ###Philippines### said yesterday it will t...</td>\n",
       "      <td>[{'mentions': 'philippines', 'wiki_id': 23440,...</td>\n",
       "      <td>90</td>\n",
       "      <td>[{'mentions': 'philippines', 'wiki_id': 23440,...</td>\n",
       "      <td>25</td>\n",
       "      <td>23440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>Philippines seeks United Nations arbitration o...</td>\n",
       "      <td>1</td>\n",
       "      <td>(87, 101)</td>\n",
       "      <td>31769</td>\n",
       "      <td>United Nations</td>\n",
       "      <td>The Philippines said yesterday it will take Ch...</td>\n",
       "      <td>[{'mentions': 'united nations', 'wiki_id': 317...</td>\n",
       "      <td>84</td>\n",
       "      <td>[{'mentions': 'united nations', 'wiki_id': 317...</td>\n",
       "      <td>54</td>\n",
       "      <td>31769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          article_text        date  \\\n",
       "0    Homebush Bay, New South Wales —Earlier today, ...  2012-07-20   \n",
       "2    Homebush Bay, New South Wales —Earlier today, ...  2012-07-20   \n",
       "4    Homebush Bay, New South Wales —Earlier today, ...  2012-07-20   \n",
       "5    Border patrols at Britain's airports may be le...  2012-07-20   \n",
       "8    Homebush Bay, New South Wales —Earlier today, ...  2012-07-21   \n",
       "..                                                 ...         ...   \n",
       "495  The Philippines said yesterday it will take Ch...  2013-01-23   \n",
       "496  The Philippines said yesterday it will take Ch...  2013-01-23   \n",
       "497  The Philippines said yesterday it will take Ch...  2013-01-23   \n",
       "498  The Philippines said yesterday it will take Ch...  2013-01-23   \n",
       "499  The Philippines said yesterday it will take Ch...  2013-01-23   \n",
       "\n",
       "                                         article_title  entity_salience  \\\n",
       "0    Australian Gliders glide past China women's na...                1   \n",
       "2    Australian Gliders glide past China women's na...                1   \n",
       "4    Australian Gliders glide past China women's na...                1   \n",
       "5              UK border officers go on Olympic strike                1   \n",
       "8    China women's national wheelchair basketball t...                1   \n",
       "..                                                 ...              ...   \n",
       "495  Philippines seeks United Nations arbitration o...                1   \n",
       "496  Philippines seeks United Nations arbitration o...                1   \n",
       "497  Philippines seeks United Nations arbitration o...                1   \n",
       "498  Philippines seeks United Nations arbitration o...                1   \n",
       "499  Philippines seeks United Nations arbitration o...                1   \n",
       "\n",
       "        offsets  wiki_ID           entity_title  \\\n",
       "0      (50, 60)  4689264              Australia   \n",
       "2      (78, 85)     5405                  China   \n",
       "4      (14, 29)    21654        New South Wales   \n",
       "5    (584, 597)   419342          David Cameron   \n",
       "8    (137, 158)   848348  wheelchair basketball   \n",
       "..          ...      ...                    ...   \n",
       "495  (782, 787)   106539                  ASEAN   \n",
       "496  (771, 777)    25734                 Taiwan   \n",
       "497  (194, 209)    74209        South China Sea   \n",
       "498     (4, 15)    23440            Philippines   \n",
       "499   (87, 101)    31769         United Nations   \n",
       "\n",
       "                                   surrounding_context  \\\n",
       "0    Homebush Bay, New South Wales —Earlier today, ...   \n",
       "2    Homebush Bay, New South Wales —Earlier today, ...   \n",
       "4    Homebush Bay, ###New South Wales### —Earlier t...   \n",
       "5    PCS members working in the Home Office, which ...   \n",
       "8    Homebush Bay, New South Wales —Earlier today, ...   \n",
       "..                                                 ...   \n",
       "495  Disputes such as those involving the Scarborou...   \n",
       "496  Disputes such as those involving the Scarborou...   \n",
       "497  The Philippines said yesterday it will take Ch...   \n",
       "498  The ###Philippines### said yesterday it will t...   \n",
       "499  The Philippines said yesterday it will take Ch...   \n",
       "\n",
       "                                            candidates  pre_pt_len_candidates  \\\n",
       "0    [{'mentions': 'australia', 'wiki_id': 4689264,...                     93   \n",
       "2    [{'mentions': 'china', 'wiki_id': 5405, 'title...                     85   \n",
       "4    [{'mentions': 'new south wales', 'wiki_id': 21...                     76   \n",
       "5    [{'mentions': 'david cameron', 'wiki_id': 4193...                     11   \n",
       "8    [{'mentions': 'wheelchair basketball', 'wiki_i...                     23   \n",
       "..                                                 ...                    ...   \n",
       "495  [{'mentions': 'asean', 'wiki_id': 28741, 'titl...                      8   \n",
       "496  [{'mentions': 'taiwan', 'wiki_id': 25734, 'tit...                     84   \n",
       "497  [{'mentions': 'south china sea', 'wiki_id': 74...                      4   \n",
       "498  [{'mentions': 'philippines', 'wiki_id': 23440,...                     90   \n",
       "499  [{'mentions': 'united nations', 'wiki_id': 317...                     84   \n",
       "\n",
       "                            candidates_after_pointwise  \\\n",
       "0    [{'mentions': 'australia', 'wiki_id': 4689264,...   \n",
       "2    [{'mentions': 'china', 'wiki_id': 5405, 'title...   \n",
       "4    [{'mentions': 'new south wales', 'wiki_id': 21...   \n",
       "5    [{'mentions': 'david cameron', 'wiki_id': 4193...   \n",
       "8    [{'mentions': 'wheelchair basketball', 'wiki_i...   \n",
       "..                                                 ...   \n",
       "495  [{'mentions': 'asean', 'wiki_id': 28741, 'titl...   \n",
       "496  [{'mentions': 'taiwan', 'wiki_id': 25734, 'tit...   \n",
       "497  [{'mentions': 'south china sea', 'wiki_id': 74...   \n",
       "498  [{'mentions': 'philippines', 'wiki_id': 23440,...   \n",
       "499  [{'mentions': 'united nations', 'wiki_id': 317...   \n",
       "\n",
       "     post_pt_len_candidates  top_linked_entity  \n",
       "0                        27           20611325  \n",
       "2                        35             887850  \n",
       "4                        37              21654  \n",
       "5                         9             419342  \n",
       "8                        19             848348  \n",
       "..                      ...                ...  \n",
       "495                       6             106539  \n",
       "496                      51              25734  \n",
       "497                       3              74209  \n",
       "498                      25              23440  \n",
       "499                      54              31769  \n",
       "\n",
       "[420 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['top_linked_entity'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a7632c-b394-402b-8f7a-ca89142fd886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Output saved to article_to_salient_entities.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2852962/3401143880.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = df1.groupby('article_title').apply(\n"
     ]
    }
   ],
   "source": [
    "grouped = df1.groupby('article_title').apply(\n",
    "    lambda group: list(\n",
    "        group[['entity_title', 'top_linked_entity']].to_dict(orient='records')\n",
    "    )\n",
    ").reset_index(name='salient_entities')\n",
    "\n",
    "grouped.to_json(\"outputs/article_to_salient_entities.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"Done. Output saved to article_to_salient_entities.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2236243-6787-4065-84f8-db2f809441b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    INSTRUCTION_PROMPT = \"1. Context: Look at the surrounding text to understand the topic.\\n2. Categories: Consider the type of the entity (person, organization, location, etc.).\\n3. Modifiers: Pay attention to words or phrases that add details to the mention.\\n4. Co-references: Check other mentions of the same entity in the text.\\n5. Temporal and Geographical Factors: Consider when and where the text was written.\\n6. External Knowledge: Use knowledge from outside the text.\\nRemember, effective entity disambiguation requires understanding the text thoroughly, having world knowledge, and exercising good judgment.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4661c0-cbf9-4254-add1-42fac7f040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_el_prompt(entity, candidates, CoT_POOL=COT_POOL, INSTRUCTION_PROMPT=INSTRUCTION_PROMPT):\n",
    "    ex = random.choice(COT_POOL)\n",
    "    \n",
    "    # CoT exemplar\n",
    "    ex_ctx = f\"{ex['left_context']} ###{ex['mention']}### {ex['right_context']}\"\n",
    "    ex_block = textwrap.dedent(f\"\"\"\\\n",
    "        The following example illustrates the task:\n",
    "        Mention: {ex['mention']}\n",
    "        Context: {ex_ctx}\n",
    "        Candidates: {ex['candidates']}\n",
    "        Answer: {ex['answer']}\"\"\")\n",
    "    \n",
    "    # create candidate map for later reconcilation\n",
    "    # cand_map, target_lines = {}, []\n",
    "    # for idx, cand in enumerate(random.sample(entity['candidates'], len(entity['candidates'])), 1):\n",
    "    #     cand_map[idx] = cand\n",
    "    #     target_lines.append(f\"Entity {idx}: {cand['cand_name']}. {cand['cand_summary']}\")\n",
    "    label_map = {}                         # label → candidate‑dict\n",
    "    cand_lines = []\n",
    "    for lbl, cand in enumerate(random.sample(entity['candidates'], len(entity['candidates'])), 1):\n",
    "        cand_lines.append(f\"{lbl}. {cand['cand_name']} – {cand['cand_summary']}\")\n",
    "        label_map[lbl] = cand              # store mapping\n",
    "        cand['prompt_label'] = lbl         # optional: keep inside dict\n",
    "\n",
    "    tgt_ctx = (entity['left_context'].strip() + ' ###' + entity['entity_title'] + '### ' + entity['right_context'].strip())\n",
    "    tgt_cand_lines = []\n",
    "    for idx, cand in enumerate(random.sample(entity['candidates'], len(entity['candidates'])), 1):\n",
    "        tgt_cand_lines.append(f\"{idx}. {cand['cand_name']} – {cand['cand_summary']}\")\n",
    "\n",
    "    tgt_block = textwrap.dedent(f\"\"\"\\\n",
    "        Now I will give you a new mention, its context, and a list of candidate entities.\n",
    "        The mention is highlighted with '###'.\n",
    "\n",
    "        Mention: {entity['entity_title']}\n",
    "        Context: {tgt_ctx}\n",
    "        {'; '.join(tgt_cand_lines)}\n",
    "\n",
    "        Think step by step.  At the end output exactly one line with the ID\n",
    "        and name of the chosen entity, e.g.  '3.Barack Obama'.\n",
    "        If none fit, output '-1.None'.\n",
    "    \"\"\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        {SYSTEM_PROMPT}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        {INSTRUCTION_PROMPT}{ex_block}{tgt_block}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    return prompt.strip(), label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a35c3a-3c1c-4aaf-91ce-388ae4b3ac09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pointwise_sed_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m cx_prompt_records \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m art_idx, art \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mpointwise_sed_outputs\u001b[49m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ent_idx, ent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(art[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ent\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pointwise_sed_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_PROMPTS = 7500\n",
    "# (art_idx, ent_idx, cand_map, prompt)\n",
    "cx_prompt_records = []\n",
    "count=0\n",
    "for art_idx, art in enumerate(pointwise_sed_outputs):\n",
    "    for ent_idx, ent in enumerate(art[\"entities\"]):\n",
    "        if not ent.get(\"candidates\"):\n",
    "            count+=1\n",
    "            continue\n",
    "\n",
    "        if len(cx_prompt_records) >= MAX_PROMPTS:\n",
    "            break\n",
    "\n",
    "        prompt, cmap = contextual_el_prompt(ent, ent.get(\"candidates\"))\n",
    "        cx_prompt_records.append((art_idx, ent_idx, cmap, prompt))\n",
    "\n",
    "    if len(cx_prompt_records) >= MAX_PROMPTS:\n",
    "        break\n",
    "        \n",
    "print(f\"Collected {len(cx_prompt_records)} prompts \"\n",
    "      f\"(last = {cx_prompt_records[-1][:2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12110aa7-0092-4fa9-b6e2-d0616b0884dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db28cb3-eb09-46dc-a861-582dd8c0ed27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cx_prompt_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf80bc-150e-45b2-84e3-ee79d2cc1a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts=prompts, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fd47b-4d16-4600-8868-ec0f36fd4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_pat = re.compile(r'(-?\\d+)\\s*\\.(.+)')  \n",
    "# for rec, out in zip(cx_prompt_records, outputs):\n",
    "#     art_idx, ent_idx, cmap, _ = rec\n",
    "#     text = out.outputs[0].text.strip()\n",
    "\n",
    "#     # --- grab the first \"<id>.<name>\" we see (top‑to‑bottom)\n",
    "#     m = answer_pat.search(text)\n",
    "#     if not m:\n",
    "#         chosen_id = -1\n",
    "#     else:\n",
    "#         chosen_id = int(m.group(1))\n",
    "\n",
    "#     # --- write back\n",
    "#     ent = pointwise_sed_outputs[art_idx][\"entities\"][ent_idx]\n",
    "#     if chosen_id in cmap:                   # LLM picked a valid label\n",
    "#         ent[\"candidates\"] = [cmap[chosen_id]]\n",
    "#         ent[\"linker_response\"] = text       # (optional, for inspection)\n",
    "#     else:                                   # -1.None  or invalid label\n",
    "#         ent[\"candidates\"] = []\n",
    "#         ent[\"linker_response\"] = text\n",
    "\n",
    "answer_pat = re.compile(r'(-?\\d+)\\s*\\.\\s*(.+)', re.I)   # e.g.  3.Barack Obama\n",
    "\n",
    "for record, output in zip(cx_prompt_records, outputs):\n",
    "    art_idx, ent_idx, label_map, _ = record\n",
    "    ent   = pointwise_sed_outputs[art_idx][\"entities\"][ent_idx]\n",
    "\n",
    "    # raw text\n",
    "    linker_resp = output.outputs[0].text.strip()\n",
    "    ent[\"linker_response\"] = linker_resp\n",
    "\n",
    "    # default: no prediction\n",
    "    ent[\"top_linked_entity\"] = None\n",
    "\n",
    "    # grab the last non‑empty line and parse \"<label>.<name>\"\n",
    "    lines = [ln.strip() for ln in linker_resp.splitlines() if ln.strip()]\n",
    "    m     = answer_pat.search(lines[-1]) if lines else None\n",
    "    if not m:                         # failed to parse → leave None\n",
    "        continue\n",
    "\n",
    "    lbl = int(m.group(1))\n",
    "    if lbl < 0 or lbl not in label_map:\n",
    "        continue                      # \"-1.None\" or invalid label\n",
    "\n",
    "    cand = label_map[lbl]             # candidate dict chosen by the model\n",
    "    ent[\"top_linked_entity\"] = {\n",
    "        \"cand_name\":    cand[\"cand_name\"],\n",
    "        \"cand_wiki_id\": cand[\"cand_wiki_id\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bffac35-83ff-48ef-82e6-238fb0111a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"outputs/pointwise/pointwise_iteration_curr/contextual_el_sed_outputs_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pointwise_sed_outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341335dc-743b-4946-827e-3850f0830dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linking(data):\n",
    "    total = 0        # every entity\n",
    "    linked = 0        # entities with a prediction\n",
    "    gts = 0        # entities that have a gold wiki_ID\n",
    "    correct = 0\n",
    "\n",
    "    for art in data:\n",
    "        for ent in art[\"entities\"]:\n",
    "            total += 1\n",
    "            gt = ent.get(\"entity wiki_ID\") or None\n",
    "            if gt:\n",
    "                gts += 1\n",
    "\n",
    "            pred_id = (ent.get(\"top_linked_entity\") or {}).get(\"cand_wiki_id\")\n",
    "            if pred_id:\n",
    "                linked += 1\n",
    "\n",
    "            if gt and pred_id and gt == pred_id:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy  = correct / total if total else 0\n",
    "    precision = correct / linked if linked else 0\n",
    "    recall    = correct / gts if gts else 0\n",
    "\n",
    "    print(f\"Entities evaluated : {total}\")\n",
    "    print(f\"Ground truths non‑null : {gts}\")\n",
    "    print(f\"Predictions made : {linked}\")\n",
    "    print(f\"Correct links : {correct}\\n\")\n",
    "\n",
    "    print(f\"Accuracy : {accuracy:.4%}\")\n",
    "    print(f\"Precision @ linked : {precision:.4%}\")\n",
    "    print(f\"Recall : {recall:.4%}\")\n",
    "\n",
    "evaluate_linking(pointwise_sed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d326424-d8c3-479c-b0ea-824539a92037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline-salient-entities",
   "language": "python",
   "name": "baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
