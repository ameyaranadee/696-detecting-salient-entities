{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2208e2-14e6-4801-8561-9f8c51554390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47dbe2-47be-49d1-8dd1-2c8dc2a5efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"work/pi_wenlongzhao_umass_edu/8/tej/696-detecting-salient-entities/data/article_info.json\"\n",
    "file_path = \"./data/article_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6aedf5-47e5-4a95-ab16-8147c90773a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455e49e-3861-49e0-b878-7c0db0d2530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['/datasets/ai/llama3/meta-llama/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6','datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa','datasets/ai/t5/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cf0b0-87a2-4071-8f1d-d965864a62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = []\n",
    "contexts = list(df['text']+\" \"+df['title'])\n",
    "prompt = '''\n",
    "You are an editor for a newspaper who has to identify the most critical pieces of\n",
    ",→ information when writing the headline for an article.\n",
    "For this task you are given a question-answer pair as Context and a list of\n",
    "entities from the text. Read the Context given in triple backticks and rate\n",
    "how salient each entity is to the Context. Before answering provide a short\n",
    "justification for your answer.\n",
    ",→\n",
    ",→\n",
    ",→\n",
    "Provide a salience score in the range of 0 to 10 where 0 is least salient and 10 is\n",
    ",→ the most salient.\n",
    "Provide a categorical rating from the following options:\n",
    "High - The entity is strongly related to the main point of the question-answer\n",
    ",→ pair or is the answer itself.\n",
    "Moderate - The entity is related to the question-answer pair but it is not the\n",
    ",→ most important part.\n",
    "Low - The entity not related or is only tangentially or superficially related\n",
    ",→ to the question-answer pair.\n",
    "Countries (especially in reference to nationality) are frequently incidental to the\n",
    "answer and are most often “Low” salience unless directly related to the\n",
    "question.\n",
    ",→\n",
    ",→\n",
    "Give your answer as valid JSON in the following format:\n",
    "[\n",
    "{{\n",
    "\"entity\": <entity_name>,\n",
    "\"explanation\": <explanation of the rating>,\n",
    "\"rating\": <rating>,\n",
    "\"score\": <score>,\n",
    "}}\n",
    "]\n",
    "Context: ```{context_str}```\n",
    "List of entities: {entity_str}\n",
    "Answer:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec6a82-92a7-4826-bf7e-d99067b4c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "####RUNS ZERO SHOT FOR EVERY MODEL SPECIFIED IN MODEL PATHS###########\n",
    "for model_path in model_paths[2:]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")  # Move model to CUDA\n",
    "    for context_str in contexts[:5]:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "        # Generate response\n",
    "        output_ids = model.generate(input_ids, max_length=500)\n",
    "\n",
    "        # Decode and print the response\n",
    "        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        outputs.append(response)\n",
    "\n",
    "print(outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885cd9a-5b11-4cfe-9beb-e027c8f8f94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b607a-971f-4c98-a2e9-f04f1568633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######TEST FOR ONE ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65abf91-4562-43f8-9a41-10b6e3c76013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an editor for a newspaper who has to identify the most critical pieces of\n",
      ",→ information when writing the headline for an article.\n",
      "For this task you are given a question-answer pair as Context and a list of\n",
      "entities from the text. Read the Context given in triple backticks and rate\n",
      "how salient each entity is to the Context. Before answering provide a short\n",
      "justification for your answer.\n",
      ",→\n",
      ",→\n",
      ",→\n",
      "Provide a salience score in the range of 0 to 10 where 0 is least salient and 10 is\n",
      ",→ the most salient.\n",
      "Provide a categorical rating from the following options:\n",
      "High - The entity is strongly related to the main point of the question-answer\n",
      ",→ pair or is the answer itself.\n",
      "Moderate - The entity is related to the question-answer pair but it is not the\n",
      ",→ most important part.\n",
      "Low - The entity not related or is only tangentially or superficially related\n",
      ",→ to the question-answer pair.\n",
      "Countries (especially in reference to nationality) are frequently incidental to the\n",
      "answer and are most often “Low” salience unless directly related to the\n",
      "question.\n",
      ",→\n",
      ",→\n",
      "Give your answer as valid JSON in the following format:\n",
      "[\n",
      "{{\n",
      "\"entity\": <entity_name>,\n",
      "\"explanation\": <explanation of the rating>,\n",
      "\"rating\": <rating>,\n",
      "\"score\": <score>,\n",
      "}}\n",
      "]\n",
      "Context: ```{context_str}```\n",
      "List of entities: {entity_str}\n",
      "Answer:\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"United States\",\n",
      "    \"explanation\": \"The United States is a country.\",\n",
      "    \"rating\": \"High\",\n",
      "    \"score\": 10,\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"World War I\",\n",
      "    \"explanation\": \"World War I was a global conflict.\",\n",
      "    \"rating\": \"Moderate\",\n",
      "    \"score\": 5,\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"John Doe\",\n",
      "    \"explanation\": \"John Doe is a person.\",\n",
      "    \"rating\": \"Low\",\n",
      "    \"score\": 2,\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"New York\",\n",
      "    \"explanation\": \"New York is a city.\",\n",
      "    \"rating\": \"Low\",\n",
      "    \"score\": 1,\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"The White House\",\n",
      "    \"explanation\": \"The White House is a building in Washington D.C.\",\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "context_str = '''The United States capital of Washington, D.C. legalized same-sex marriage on Wednesday. Beginning at 6 A.M. local time (1100 UTC), couples began submitting marriage applications at local courthouses citywide.Washington D.C. becomes the seventh United States territory to legalize same sex marriage. The bill was ratified by Mayor Adrian Fenty last December. Due to city's territorial status as a federal district, the bill had to be reviewed by congress. The bill passed congressional review Tuesday night.The bill faced opposition from many family values activists, who tried to stop the bill from becoming law. Supreme Court Chief Justice John Roberts rejected a lawsuit to prevent the measure.'''\n",
    "prompt = '''\n",
    "You are an editor for a newspaper who has to identify the most critical pieces of\n",
    ",→ information when writing the headline for an article.\n",
    "For this task you are given a question-answer pair as Context and a list of\n",
    "entities from the text. Read the Context given in triple backticks and rate\n",
    "how salient each entity is to the Context. Before answering provide a short\n",
    "justification for your answer.\n",
    ",→\n",
    ",→\n",
    ",→\n",
    "Provide a salience score in the range of 0 to 10 where 0 is least salient and 10 is\n",
    ",→ the most salient.\n",
    "Provide a categorical rating from the following options:\n",
    "High - The entity is strongly related to the main point of the question-answer\n",
    ",→ pair or is the answer itself.\n",
    "Moderate - The entity is related to the question-answer pair but it is not the\n",
    ",→ most important part.\n",
    "Low - The entity not related or is only tangentially or superficially related\n",
    ",→ to the question-answer pair.\n",
    "Countries (especially in reference to nationality) are frequently incidental to the\n",
    "answer and are most often “Low” salience unless directly related to the\n",
    "question.\n",
    ",→\n",
    ",→\n",
    "Give your answer as valid JSON in the following format:\n",
    "[\n",
    "{{\n",
    "\"entity\": <entity_name>,\n",
    "\"explanation\": <explanation of the rating>,\n",
    "\"rating\": <rating>,\n",
    "\"score\": <score>,\n",
    "}}\n",
    "]\n",
    "Context: ```{context_str}```\n",
    "List of entities: {entity_str}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "# Tokenize the prompt and move input to CUDA\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "output_ids = model.generate(input_ids, max_length=500)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a86086-cf9d-44fb-a381-998839ab6d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tej_baseline",
   "language": "python",
   "name": "tej_baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
